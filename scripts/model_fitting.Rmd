---
title: "Model Fitting"
author: "Jiashu Liu"
date: "2024-06-03"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Load libraries
library(tidyverse)
library(dplyr)
library(ROCR)
library(ranger)
library(randomForest)
#library(pak)
#pak::pak("caret")
library(caret)
library(e1071)
library(nnet)
library(dummy)
library(gbm)
```

```{r}
# Load in dataset
NSDUH_2022 <- read.csv("/Users/jiashuliu/Desktop/Projects/substance_use_disorder/data/sud_2022.csv")
# Convert all the variables into factors
NSDUH_2022 <- NSDUH_2022 %>% 
  mutate(across(where(is.numeric), as.factor))
str(NSDUH_2022)
```
```{r}
# NSDUH_2022 is an imbalanced dataset
table(NSDUH_2022$SUD_MJ)
sud_mj_yes <- subset(NSDUH_2022, SUD_MJ == 1)
sud_mj_no <- subset(NSDUH_2022, SUD_MJ == 0)
ratio_yes <- nrow(sud_mj_yes) / nrow(NSDUH_2022)
ratio_no <- nrow(sud_mj_no) / nrow(NSDUH_2022)

# Display the ratios
cat("Ratio of SUD_MJ = Yes:", round(ratio_yes, 2), "\n")
cat("Ratio of SUD_MJ = No:", round(ratio_no, 2), "\n")
```

### Split Training and Testing Set 
```{r}
set.seed(123)
# find split_size to divide data in 67% train/ 33% test sets
split_size <- sample(1:nrow(NSDUH_2022), floor(0.67 * nrow(NSDUH_2022)))

# Extract the train and test sets
train <- NSDUH_2022[split_size, ]
test <- NSDUH_2022[-split_size, ]
# Change the levels of SUD_MJ from 0 and 1 to 'No' and 'Yes'.
# Otherwise this will lead to errors in trainControl
levels(train$SUD_MJ) <- c("No", "Yes")
levels(test$SUD_MJ) <- c("No", "Yes")
levels(train$SUD_MJ)
levels(test$SUD_MJ)
# Stratified cross-validation
trControl <- trainControl(method = "cv", 
                          number = 10, 
                          classProbs = TRUE, 
                          summaryFunction = twoClassSummary)
```

### Logistic Regression
```{r}
# Train logistic regression model on training set
set.seed(123)
logistic_cv <- caret::train(SUD_MJ ~ ., 
                                  data = train, 
                                  method = "glm", 
                                  family = binomial, 
                                  trControl = trControl, # using cv to avoid overfitting
                                  metric = "ROC")
print(logistic_cv)

# Make predictions on the testing set
test$predicted_prob_logistic <- predict(logistic_cv, newdata = test, type = "prob")[, "Yes"]
test$predicted_class_logistic <- ifelse(test$predicted_prob_logistic > 0.5, "Yes", "No")
test$predicted_class_logistic <- factor(test$predicted_class_logistic, levels = c("No", "Yes"))
```
## Model Performance 
```{r}
confusion_matrix_logistic <- confusionMatrix(test$predicted_class_logistic, test$SUD_MJ)
print(confusion_matrix_logistic)
# accuracy
accuracy_logi <- confusion_matrix_logistic$overall['Accuracy']
print(paste("Accuracy:", round(accuracy_logi, 2)))
# Precision, Recall, F1-Score
precision_logi <- confusion_matrix_logistic$byClass['Pos Pred Value']
recall_logi <- confusion_matrix_logistic$byClass['Sensitivity'] 
F1_score_logi <- 2 * ((precision_logi * recall_logi) / (precision_logi + recall_logi))
print(paste("Precision:", round(precision_logi, 2)))
print(paste("Recall:", round(recall_logi, 2)))
print(paste("F1-Score:", round(F1_score_logi, 2)))
# AUC
pred <- prediction(test$predicted_prob, test$SUD_MJ)
perf_auc <- performance(pred, measure = "auc")
auc_value <- perf_auc@y.values[[1]]
print(paste("AUC:", round(auc_value, 2)))
perf_roc <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf_roc, main = "ROC Curve", col = "red", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "gray")
```

### Random Forest
```{r}
# Train random forest model using training data 
set.seed(123)
rf_model<- randomForest(SUD_MJ ~ ., 
                        data = train, 
                        ntree = 500, 
                        importance = TRUE)
# Tune the Rf model by finding the optimal mtry value 
# Select mtry value with minimum out of bag(OOB) error
mtry <- tuneRF(train[, -ncol(train)],train$SUD_MJ, ntreeTry=500,
               stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE)
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
print(mtry)
print(best.m)
```

```{r}
# Refit the model using best mtry
rf_tuned <- randomForest(SUD_MJ ~., data = train, ntree = 500, mtry = best.m, importance=TRUE)
print(rf_tuned)
importance(rf_tuned) 
varImpPlot(rf_tuned)
# Higher the value of mean decrease accuracy or mean decrease gini score , higher the importance of the variable in the model. In the plot shown blow, mental health is most important variable.
# Mean Decrease Accuracy - How much the model accuracy decreases if we drop that variable.
# Mean Decrease Gini - Measure of variable importance based on the Gini impurity index used for the calculation of splits in trees.
# Making predictions based on test data
test$pred_prob_rf_tuned <- predict(rf_tuned, newdata = test, type = "prob")[, 2]
test$pred_class_rf_tuned <- ifelse(test$pred_prob_rf_tuned > 0.5, 1, 0)
confusion_matrix_rf_tuned <- confusionMatrix(as.factor(test$pred_class_rf_tuned), test$SUD_MJ)
print(confusion_matrix_rf_tuned)
```
```{r}
# Accuracy
accuracy_rf_tuned <- confusion_matrix_rf_tuned$overall['Accuracy']
print(paste("Accuracy:", round(accuracy_rf_tuned, 2)))
# Precision, Recall, F1-Score
precision_rf_tuned <- confusion_matrix_rf_tuned$byClass['Pos Pred Value']
recall_rf_tuned <- confusion_matrix_rf_tuned$byClass['Sensitivity']
F1_rf_tuned <- 2 * ((precision_rf_tuned * recall_rf_tuned) / (precision_rf_tuned + recall_rf_tuned))
print(paste("Precision:", round(precision_rf_tuned, 2)))
print(paste("Recall:", round(recall_rf_tuned, 2)))
print(paste("F1-Score:", round(F1_rf_tuned, 2)))
# AUC
pred_rf_tuned <- prediction(test$pred_prob_rf_tuned, test$SUD_MJ)
auc_rf_tuned <- performance(pred_rf_tuned, "auc")
auc_value_rf_tuned <- auc_rf_tuned@y.values[[1]]
print(paste("AUC:", round(auc_value_rf_tuned, 2)))
roc_rf_tuned <- performance(pred_rf_tuned, measure = "tpr", x.measure = "fpr")
plot(roc_rf_tuned, main="ROC Curve for Random Forest",col=2,lwd=2)
abline(a=0,b=1,lwd=2,lty=2,col="gray")
```

### Neural Network
```{r}
head(train)
```

```{r}
# One-hot encode using model.matrix
train_matrix <- model.matrix(SUD_MJ ~ . - 1, data = train)
test_matrix <- model.matrix(SUD_MJ ~ . - 1, data = test)

# Convert the results to data frames
train_encoded <- as.data.frame(train_matrix)
test_encoded <- as.data.frame(test_matrix)

# Ensure the target variable SUD_MJ is included
train_encoded$SUD_MJ <- train$SUD_MJ
test_encoded$SUD_MJ <- test$SUD_MJ

# Identify numeric columns
numeric_cols_train <- sapply(train_encoded, is.numeric)
numeric_cols_test <- sapply(test_encoded, is.numeric)

# Apply normalization (scaling)
train_encoded[numeric_cols_train] <- scale(train_encoded[numeric_cols_train])
test_encoded[numeric_cols_test] <- scale(test_encoded[numeric_cols_test])

# Verify column names again after scaling
identical(names(train_encoded), names(test_encoded))
```

```{r}
set.seed(123)
nn.fit <- caret::train(SUD_MJ ~ ., 
                       data = train_encoded, 
                       method = "nnet", 
                       trControl = trControl, 
                       trace = FALSE, 
                       metric = "ROC", 
                       linout = FALSE, 
                       maxit = 200)

# Print the model summary
print(nn.fit)
```
```{r}
# Make prediction using testing set
test_encoded$predicted_prob_nn <- predict(nn.fit, newdata = test_encoded, type = "prob")[, "Yes"]
test_encoded$predicted_class_nn <- ifelse(test_encoded$predicted_prob_nn > 0.5, "Yes", "No")
test_encoded$predicted_class_nn <- factor(test_encoded$predicted_class_nn, levels = c("No", "Yes"))
confusion_matrix_nn <- confusionMatrix(test_encoded$predicted_class_nn, test_encoded$SUD_MJ)
print(confusion_matrix_nn)
# Calculate and print performance metrics for the neural network model on the testing set

accuracy_test_nn <- confusion_matrix_nn$overall['Accuracy']
print(paste("Accuracy:", round(accuracy_test_nn, 2)))

precision_test_nn <- confusion_matrix_nn$byClass['Pos Pred Value']
recall_test_nn <- confusion_matrix_nn$byClass['Sensitivity']
f1_score_test_nn <- 2 * ((precision_test_nn * recall_test_nn) / (precision_test_nn + recall_test_nn))

print(paste("Precision:", round(precision_test_nn, 2)))
print(paste("Recall:", round(recall_test_nn, 2)))
print(paste("F1-Score:", round(f1_score_test_nn, 2)))

# Using ROCR to calculate AUC and plot ROC curve for the neural network model
pred_nn <- prediction(test_encoded$predicted_prob_nn, test_encoded$SUD_MJ)
perf_auc_nn <- performance(pred_nn, measure = "auc")
auc_value_nn <- perf_auc_nn@y.values[[1]]
print(paste("AUC:", round(auc_value_nn, 2)))

# Performance object for ROC
perf_roc_nn <- performance(pred_nn, measure = "tpr", x.measure = "fpr")

# Plot the ROC curve for the neural network model
plot(perf_roc_nn, main = "ROC Curve for Neural Network", col = "blue", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "gray")
```

### Gradient Boosting Model

```{r}
# Set the seed for reproducibility
set.seed(123)
# 10-folds cross-validation to prevent overfitting
trControl <- trainControl(method = "cv", 
                          number = 10, 
                          classProbs = TRUE, 
                          summaryFunction = twoClassSummary,
                          search = "grid")
# tune the hyperparameters
tuneGrid <- expand.grid(interaction.depth = c(1, 3, 5),  # Depth of each tree
                        n.trees = c(50, 100, 150),       # Number of trees
                        shrinkage = c(0.01, 0.1, 0.3),   # Learning rate
                        n.minobsinnode = c(10, 20))
# fit gbm model with train data
gbm_model <- caret::train(SUD_MJ ~ ., 
                          data = train_encoded, 
                          method = "gbm", 
                          trControl = trControl, 
                          tuneGrid = tuneGrid, 
                          metric = "ROC", 
                          verbose = FALSE)
print(gbm_model)
```

```{r}
best_params <- gbm_model$bestTune
print(best_params)
```

```{r}
# Convert target variable to numeric (0 and 1)
train_encoded$SUD_MJ <- ifelse(train_encoded$SUD_MJ == "Yes", 1, 0)
test_encoded$SUD_MJ <- ifelse(test_encoded$SUD_MJ == "Yes", 1, 0)

gbm_best <- gbm::gbm(SUD_MJ ~ ., 
                      data = train_encoded, 
                      distribution = "bernoulli", 
                      n.trees = best_params$n.trees, 
                      interaction.depth = best_params$interaction.depth, 
                      shrinkage = best_params$shrinkage, 
                      n.minobsinnode = best_params$n.minobsinnode, 
                      cv.folds = 10, 
                      keep.data = TRUE, 
                      verbose = FALSE)
summary(gbm_best)
```


```{r}
# Make predictions on the testing set
test_encoded$predicted_prob_gbm <- predict(gbm_best, newdata = test_encoded, n.trees = best_params$n.trees, type = "response")
test_encoded$predicted_class_gbm <- ifelse(test_encoded$predicted_prob_gbm > 0.5, 1, 0)

# Confusion matrix to evaluate the GBM model on the testing set
confusion_matrix_test_gbm <- confusionMatrix(as.factor(test_encoded$predicted_class_gbm), as.factor(test_encoded$SUD_MJ))
print(confusion_matrix_test_gbm)

# Calculate and print performance metrics for the GBM model on the testing set
accuracy_test_gbm <- confusion_matrix_test_gbm$overall['Accuracy']
print(paste("Accuracy:", round(accuracy_test_gbm, 2)))

precision_test_gbm <- confusion_matrix_test_gbm$byClass['Pos Pred Value']
recall_test_gbm <- confusion_matrix_test_gbm$byClass['Sensitivity']
f1_score_test_gbm <- 2 * ((precision_test_gbm * recall_test_gbm) / (precision_test_gbm + recall_test_gbm))

print(paste("Precision:", round(precision_test_gbm, 2)))
print(paste("Recall:", round(recall_test_gbm, 2)))
print(paste("F1-Score:", round(f1_score_test_gbm, 2)))

# Using ROCR to calculate AUC and plot ROC curve for the GBM model
pred_gbm <- prediction(test_encoded$predicted_prob_gbm, test_encoded$SUD_MJ)
perf_auc_gbm <- performance(pred_gbm, measure = "auc")
auc_value_gbm <- perf_auc_gbm@y.values[[1]]
print(paste("AUC:", round(auc_value_gbm, 2)))

# Performance object for ROC
perf_roc_gbm <- performance(pred_gbm, measure = "tpr", x.measure = "fpr")

# Plot the ROC curve for the GBM model
plot(perf_roc_gbm, main = "ROC Curve for GBM", col = "blue", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "gray")
```











